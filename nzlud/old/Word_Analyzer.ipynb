{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "genai.configure(api_key='AIzaSyA20vKhMldJ2soP-GBZJ6LJDVwqrdOHn7g')\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)\n",
        "model = genai.GenerativeModel('gemini-1.5-pro-latest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open and read the contents of gen_matches.txt\n",
        "with open('gen_matches.txt', 'r') as file:\n",
        "    gen_matches_content = file.read()\n",
        "\n",
        "# Now gen_matches_content contains the contents of gen_matches.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_matches_content = gen_matches_content.replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\").replace(\":\", \"\").replace(\"/\", \"\")\n",
        "gen_matches_content = gen_matches_content.replace(\"-\", \"\").replace(\"'\", \"\").replace(\",\", \"\").replace('``', '').replace(\"''\", \"\")\n",
        "gen_matches_content = gen_matches_content.replace(\"[\", \"\").replace(\"]\", \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\clint\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\clint\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Assuming gen_matches_content is a string containing the text from which you want to remove stop words\n",
        "# First, we need to import the necessary library\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the set of stop words the first time\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to remove stop words from a string\n",
        "def remove_stop_words(content):\n",
        "    # Tokenize the content into words\n",
        "    words = word_tokenize(content)\n",
        "    \n",
        "    # Get the set of English stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    # Filter out the stop words from the tokenized words\n",
        "    filtered_content = [word for word in words if not word.lower() in stop_words]\n",
        "    \n",
        "    # Join the filtered words back into a string\n",
        "    filtered_content_string = ' '.join(filtered_content)\n",
        "    \n",
        "    return filtered_content_string\n",
        "\n",
        "\n",
        "gen_matches_content = remove_stop_words(gen_matches_content)\n",
        "import re\n",
        "gen_matches_content = re.sub(r'\\d+', '', gen_matches_content) #remove numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Find the most common words in gen_matches_content\n",
        "word_counts = Counter(gen_matches_content.split())\n",
        "most_common_words = word_counts.most_common()\n",
        "\n",
        "# Divide the word counts by 27\n",
        "word_counts_divided = {word: count / 27 for word, count in most_common_words}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Define the dictionary with words and their associated counts\n",
        "words_dict = word_counts_divided\n",
        "\n",
        "# Step 2: Create a new dictionary to store the merged entries\n",
        "merged_dict = {}\n",
        "# Step 3: Create a list to keep track of words to be deleted\n",
        "words_to_delete = []\n",
        "\n",
        "# Step 3: Iterate through the original dictionary to find and merge words\n",
        "for word, count in words_dict.items():\n",
        "    # Check if the word without the trailing 's' exists in the dictionary\n",
        "    if word.endswith('s') and word[:-1] in words_dict:\n",
        "        # Create a new key for the merged word\n",
        "        merged_key = f\"{word[:-1]}/{word}\"\n",
        "        # Sum the counts of the singular and plural forms\n",
        "        merged_count = count + words_dict[word[:-1]]\n",
        "        # Add or update the entry in the merged dictionary\n",
        "        merged_dict[merged_key] = merged_count\n",
        "        # Add the original words to the list of words to be deleted\n",
        "        words_to_delete.append(word)\n",
        "        words_to_delete.append(word[:-1])\n",
        "    elif word.endswith('ing') and word[:-3] in words_dict:\n",
        "        # Create a new key for the merged word\n",
        "        merged_key = f\"{word[:-3]}/{word}\"\n",
        "        # Sum the counts of the base and 'ing' form\n",
        "        merged_count = count + words_dict[word[:-3]]\n",
        "        # Add or update the entry in the merged dictionary\n",
        "        merged_dict[merged_key] = merged_count\n",
        "        # Add the original words to the list of words to be deleted\n",
        "        words_to_delete.append(word)\n",
        "        words_to_delete.append(word[:-3])\n",
        "    else:\n",
        "        merged_dict[word] = count\n",
        "# Step 5: Remove the words to be deleted from the merged dictionary\n",
        "for word in set(words_to_delete):  # Use set to ensure uniqueness\n",
        "    merged_dict.pop(word, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort the merged_dict by count in descending order\n",
        "sorted_dict = dict(sorted(merged_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Export the sorted word counts as a CSV file\n",
        "with open('sorted_word_counts.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Word', 'Count'])\n",
        "    writer.writerows(sorted_dict.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "python.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
